import streamlit as st 
import os.path,sys
sys.path.append(os.path.abspath(os.path.dirname(''))+'/../../')
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D,Input
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import ImageDataGenerator
import keras.callbacks
import cv2
import numpy as np
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.image import load_img, img_to_array
from keras.applications.vgg16 import preprocess_input
from PIL import Image
import requests
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep
import requests
from bs4 import BeautifulSoup
from keras.models import load_model
import pickle

st.set_page_config(
    page_title="Sweets&Coffee App",
    page_icon="â˜•",
    layout="wide",
    initial_sidebar_state="expanded"
)

try:
    model = load_model('1115poch5batch8wari75.hdf5')
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

sweets_classes_names = {
    0: ["ã‚¢ãƒƒãƒ—ãƒ«ãƒ‘ã‚¤"], 
    1: ["ãƒ–ãƒ«ãƒ¼ãƒ™ãƒªãƒ¼ã‚±ãƒ¼ã‚­"], 
    2: ["ã‚«ãƒŒãƒ¬"], 
    3: ["ãƒãƒ§ã‚³ã‚±ãƒ¼ã‚­"], 
    4: ["ãƒãƒ§ã‚³ãƒãƒƒãƒ—ã‚¯ãƒƒã‚­ãƒ¼"], 
    5: ["ãƒ—ãƒ¬ãƒ¼ãƒ³ã‚¯ãƒƒã‚­ãƒ¼"], 
    6: ["ã‚·ãƒ¥ãƒ¼ã‚¯ãƒªãƒ¼ãƒ "], 
    7: ["ã‚¨ã‚¯ãƒ¬ã‚¢"], 
    8: ["ãƒ•ãƒ«ãƒ¼ãƒ„ã‚¿ãƒ«ãƒˆ"], 
    9: ["ãƒ–ãƒ‰ã‚¦ã‚±ãƒ¼ã‚­"], 
    10: ["ãƒ¬ãƒ¢ãƒ³ãƒ‘ã‚¤"], 
    11: ["ãƒŠãƒƒãƒ„ã‚¿ãƒ«ãƒˆ"], 
    12: ["ã‚‚ã‚‚ã®ã‚±ãƒ¼ã‚­"], 
    13: ["ã‚¤ãƒã‚´ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚±ãƒ¼ã‚­"], 
    14: ["ãƒ¯ãƒƒãƒ•ãƒ«"]
}

# ã‚¯ãƒ©ã‚¹åã®è¨­å®š
flavor_classes_names = {
    0: ["ã‚¢ãƒƒãƒ—ãƒ«", "æ—æª", "ãƒªãƒ³ã‚´", "ã‚Šã‚“ã”"], 
    1: ["ãƒ–ãƒ«ãƒ¼ãƒ™ãƒªãƒ¼", "ãƒ™ãƒªãƒ¼"], 
    2: ["ãƒãƒ‹ãƒ©"], 
    3: ["ãƒãƒ§ã‚³"], 
    4: ["ãƒãƒ§ã‚³"], 
    5: ["ã‚¹ã‚¤ãƒ¼ãƒˆ", "ã‚¹ã‚¦ã‚£ãƒ¼ãƒˆ", "ç”˜å‘³", "ç”˜ã•"], 
    6: ["ãƒãƒ‹ãƒ©"], 
    7: ["ãƒãƒ§ã‚³", "ãƒãƒ‹ãƒ©"], 
    8: ["ãƒ•ãƒ«ãƒ¼ãƒ†ã‚£ãƒ¼", "æœå®Ÿ"], 
    9: ["ãƒ–ãƒ‰ã‚¦", "ã¶ã©ã†", "ã‚°ãƒ¬ãƒ¼ãƒ—", "ãƒã‚¹ã‚«ãƒƒãƒˆ"], 
    10: ["ãƒ¬ãƒ¢ãƒ³", "ã‚Œã‚‚ã‚“", "æª¸æª¬", "æŸ‘æ©˜", "ã‚·ãƒˆãƒ©ã‚¹"], 
    11: ["ã‚¢ãƒ¼ãƒ¢ãƒ³ãƒ‰", "ãƒŠãƒƒãƒ„"], 
    12: ["ãƒ”ãƒ¼ãƒ", "ã‚‚ã‚‚", "é»„æ¡ƒ"], 
    13: ["ã‚¹ãƒˆãƒ­ãƒ™ãƒªãƒ¼", "ã‚¤ãƒã‚´", "ã„ã¡ã”", "è‹º", "ãƒ™ãƒªãƒ¼"], 
    14: ["èœ‚èœœ", "ã¯ã¡ã¿ã¤", "ãƒãƒ‹ãƒ¼", "ãƒ¡ãƒ¼ãƒ—ãƒ«", "ã‚«ãƒ©ãƒ¡ãƒ«"]
}

# Streamlitã‚¿ã‚¤ãƒˆãƒ«
st.title('ã‚¹ã‚¤ãƒ¼ãƒ„ã«ç›¸æ€§ãŒã‚ˆã„ã‚³ãƒ¼ãƒ’ãƒ¼ ã‚’ææ¡ˆã„ãŸã—ã¾ã™ï¼')
st.subheader('ğŸ°ã‚¹ã‚¤ãƒ¼ãƒ„ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ğŸ°')
st.caption('â€»ç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«ã‚³ãƒ¼ãƒ’ãƒ¼è±†ã‚’ææ¡ˆã„ãŸã—ã¾ã™ï¼')

# ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„...", type='jpg')

if uploaded_file is None:
    st.warning("ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

if uploaded_file is not None:
    try:
        img = Image.open(uploaded_file)
        st.image(img, caption='ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ', use_column_width=True)

        # ç”»åƒã®å‰å‡¦ç†
        IMAGE_SIZE = 224
        img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE))
        x = img_to_array(img_resized)
        x = np.expand_dims(x, axis=0)
        

        # äºˆæ¸¬
        predict = model.predict(preprocess_input(x))

        for pre in predict:
            y = pre.argmax()  # æœ€å¤§ã®äºˆæ¸¬å€¤ã‚’æŒã¤ã‚¯ãƒ©ã‚¹ã‚’å–å¾—
            flavor_class_name = flavor_classes_names[y] 
            sweets_class_name = sweets_classes_names[y]  # ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆã‚’å–å¾—
            confidence = pre[y]  # ãã®ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’å–å¾—
            

            # çµæœè¡¨ç¤º
            if confidence >= 0.7:
                st.success(f"äºˆæ¸¬çµæœ: {sweets_class_name} (ç¢ºç‡: {confidence:.2f})")
                keywords = flavor_class_name
            else:
                st.warning("ã™ã¿ã¾ã›ã‚“ã€èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        if keywords is None:
            st.error("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            st.stop()

    except Exception as e:
        st.error(f"ç”»åƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()

st.title("â˜•ãŠã™ã™ã‚ã®ã‚³ãƒ¼ãƒ’ãƒ¼è±†â˜•") 
# å„ã‚µã‚¤ãƒˆã®ãƒ«ãƒ¼ãƒˆURLã¨å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’å®šç¾©
root_urls = {
    'wataru': 'https://watarucoffee-onlinestore.com/',
    'hirocoffee': 'https://www.hirocoffee.jp',
    'tonya': 'https://www.tonya.co.jp',
    'tokiwa': 'https://tokiwacoffee.com',
    'toshino': 'http://shop.toshinocoffee.jp/'
}

urls = {
    'wataru': ['https://watarucoffee-onlinestore.com/?mode=cate&cbid=2666252&csid=0'],
    'hirocoffee': ['https://www.hirocoffee.jp/?mode=f1'],
    'tonya': ['https://www.tonya.co.jp/shop/c/c2005'],
    'tokiwa': [
        'https://tokiwacoffee.com/item_cat/beans/',
        'https://tokiwacoffee.com/item_cat/beans/page/2/',
        'https://tokiwacoffee.com/item_cat/beans/page/3/',
        'https://tokiwacoffee.com/item_cat/beans/page/4/',
        'https://tokiwacoffee.com/item_cat/beans/page/5/'
    ],
    'toshino': [
        'http://shop.toshinocoffee.jp/?mode=cate&cbid=1583225&csid=0',
        'http://shop.toshinocoffee.jp/?mode=cate&cbid=1583225&csid=0&page=2'
    ]
}

# å„ã‚µã‚¤ãƒˆã”ã¨ã®å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
all_product_urls = {
    'wataru': [],
    'hirocoffee': [],
    'tonya': [],
    'tokiwa': [],
    'toshino': []
}

# å„ã‚µã‚¤ãƒˆã‹ã‚‰å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—
for store, store_urls in urls.items():
    for url in store_urls:
        res = requests.get(url)
        if res.status_code != 200:
            print(f"Failed to retrieve {url}")
            continue
        
        soup = BeautifulSoup(res.text, 'html.parser')

        if store == 'wataru':
            beans = soup.find('ul', class_='c-product-list')
            if beans:
                product_items = beans.find_all('li', class_='c-product-list__item')
                for item in product_items:
                    product_link_tag = item.find('a')
                    if product_link_tag:
                        bean_url = product_link_tag['href']
                        product_url = root_urls[store] + bean_url
                        all_product_urls[store].append(product_url)
        
        elif store == 'hirocoffee':
            beans = soup.find('div', class_='p-item-sec-wrap')
            if beans:
                product_items = beans.find_all('div', class_='p-item-box')
                for item in product_items:
                    product_link_tag = item.find('a')
                    if product_link_tag:
                        product_url = product_link_tag['href']
                        all_product_urls[store].append(product_url)
        
        elif store == 'tonya':
            beans = soup.find('div', class_='StyleT_Line_ tile_line_')
            if beans:
                product_items = beans.find_all('div', class_='StyleT_Item_ tile_item_ C2005')
                for item in product_items:
                    product_link_tag = item.find('a')
                    if product_link_tag:
                        bean_url = product_link_tag['href']
                        product_url = root_urls[store] + bean_url
                        all_product_urls[store].append(product_url)
        
        elif store == 'tokiwa':
            beans = soup.find('div', id='main')
            if beans:
                product_items = beans.find_all('li')
                for item in product_items:
                    product_link_tag = item.find('a')
                    if product_link_tag:
                        product_url = product_link_tag['href']
                        all_product_urls[store].append(product_url)
        
        elif store == 'toshino':
            beans = soup.find('div', class_='category_items')
            if beans:
                product_items = beans.find_all('div',class_ = 'item_thumbnail')
                for item in product_items:
                    product_link_tag = item.find('a')
                    if product_link_tag:
                        bean_url = product_link_tag['href']
                        product_url = root_urls[store] + bean_url
                        all_product_urls[store].append(product_url)
                        

# è©²å½“ã™ã‚‹å•†å“ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
matching_products = []

# å„å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
for store, product_urls in all_product_urls.items():
    for product_url in product_urls:
        res = requests.get(product_url)
        if res.status_code != 200:
            #print(f"Failed to retrieve {product_url}")
            continue
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        if store == 'wataru':
            product_article = soup.find('div', class_='p-product-body')
            if product_article:
                cupping_notes = product_article.find_all('div', class_='m-table')
                for note in cupping_notes:
                    note_text_tag = note.find('tr')
                    if note_text_tag:
                        note_text = note_text_tag.get_text()
                        if any(keyword in note_text for keyword in keywords):
                            product_name_tag = product_article.find('div', class_='p-product-body__name')
                            if product_name_tag:
                                product_name = product_name_tag.get_text(strip=True)
                                matching_products.append({'URL': product_url, 'éŠ˜æŸ„': product_name})
        
        elif store == 'hirocoffee':
            product_article = soup.find('div', class_='p-product-body')
            if product_article:
                cupping_notes = product_article.find_all('div', class_='p-product-body-inner')
                for note in cupping_notes:
                    note_text_tag = note.find('p')
                    if note_text_tag:
                        note_text = note_text_tag.get_text()
                        if any(keyword in note_text for keyword in keywords):
                            product_name_tag = product_article.find('div', class_='p-product-body__name')
                            if product_name_tag:
                                product_name = product_name_tag.get_text(strip=True)
                                matching_products.append({ 'URL': product_url, 'éŠ˜æŸ„': product_name})

        elif store == 'tonya':
            product_article = soup.find('div', class_='goodsproductdetail_inner_')
            if product_article:
                cupping_notes = product_article.find_all('div', class_='goodsspec_area_', id='cart_opt')
                for note in cupping_notes:
                    note_text_tag = note.find('div', class_='goodscomment2_')
                    if note_text_tag:
                        note_text = note_text_tag.get_text()
                        if any(keyword in note_text for keyword in keywords):
                            product_name_tag = product_article.find('h2', class_='goods_rifhtname_')
                            if product_name_tag:
                                product_name = product_name_tag.get_text(strip=True)
                                matching_products.append({'URL': product_url, 'éŠ˜æŸ„': product_name})

        elif store == 'tokiwa':
            product_article = soup.find('article', class_='article_item')
            if product_article:
                cupping_notes = product_article.find_all('dl', class_='dlist_cupping')
                for note in cupping_notes:
                    note_text_tag = note.find('dd')
                    if note_text_tag:
                        note_text = note_text_tag.get_text()
                        if any(keyword in note_text for keyword in keywords):
                            product_name_tag = product_article.find('h1')
                            if product_name_tag:
                                product_name = product_name_tag.get_text(strip=True)
                                matching_products.append({'URL': product_url, 'éŠ˜æŸ„': product_name})

        elif store == 'toshino':
            product_article = soup.find('tr', valign='top')
            if product_article:
                cupping_notes = product_article.find('div', id='detail')
                if cupping_notes:
                    note_text = cupping_notes.get_text()
                    if any(keyword in note_text for keyword in keywords):
                        product_name_tag = soup.find('p', class_='pagetitle')
                        if product_name_tag:
                            product_name = product_name_tag.get_text(strip=True)
                            matching_products.append({'URL': product_url, 'éŠ˜æŸ„': product_name})

            # çµæœã®è¡¨ç¤º
if matching_products:
    df = pd.DataFrame(matching_products)
    st.table(df)
else:
    st.warning("è©²å½“ã™ã‚‹å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")





